import streamlit as st
import pandas as pd
import json
from datetime import datetime, timedelta
import os
import sys
from pathlib import Path
from utils.auth import authenticate_and_logout
import re

# Ajout du r√©pertoire parent au PYTHONPATH
sys.path.append(str(Path(__file__).parent.parent))
from utils.db_utils import init_db, save_metadata, get_types_donnees, get_producteurs_by_type, get_jeux_donnees_by_producteur, get_db_connection

def normalize_data_type(raw_type: str) -> str:
    """
    Normalise un type de donn√©es brut vers un type SQL standard.
    
    Args:
        raw_type: Type brut trouv√© dans le dictionnaire des variables
        
    Returns:
        Type SQL standardis√© ou None si non reconnu
    """
    if not raw_type:
        return None
    
    type_clean = str(raw_type).strip().lower()
    
    # Mapping des types courants vers SQL
    type_mappings = {
        # Types texte
        'text': 'TEXT',
        'string': 'VARCHAR(255)',
        'str': 'VARCHAR(255)', 
        'char': 'VARCHAR(255)',
        'character': 'VARCHAR(255)',
        'varchar': 'VARCHAR(255)',
        'texte': 'TEXT',
        'cha√Æne': 'VARCHAR(255)',
        'chaine': 'VARCHAR(255)',
        
        # Types num√©riques entiers
        'int': 'INTEGER',
        'integer': 'INTEGER',
        'entier': 'INTEGER',
        'number': 'INTEGER',
        'numeric': 'INTEGER',
        'num': 'INTEGER',
        'bigint': 'BIGINT',
        'smallint': 'SMALLINT',
        
        # Types num√©riques d√©cimaux
        'float': 'DECIMAL(15,6)',
        'decimal': 'DECIMAL(15,6)',
        'double': 'DECIMAL(15,6)',
        'real': 'DECIMAL(15,6)',
        'd√©cimal': 'DECIMAL(15,6)',
        'flottant': 'DECIMAL(15,6)',
        
        # Types date/temps
        'date': 'DATE',
        'datetime': 'TIMESTAMP',
        'timestamp': 'TIMESTAMP',
        'time': 'TIME',
        'temps': 'TIMESTAMP',
        
        # Types bool√©ens
        'boolean': 'BOOLEAN',
        'bool': 'BOOLEAN',
        'bool√©en': 'BOOLEAN',
        'vrai/faux': 'BOOLEAN',
        'oui/non': 'BOOLEAN',
        
        # Types binaires/blob
        'blob': 'BYTEA',
        'binary': 'BYTEA',
        'binaire': 'BYTEA'
    }
    
    # Recherche directe
    if type_clean in type_mappings:
        return type_mappings[type_clean]
    
    # Recherche avec VARCHAR(n)
    import re
    varchar_match = re.match(r'varchar\s*\(\s*(\d+)\s*\)', type_clean)
    if varchar_match:
        size = int(varchar_match.group(1))
        return f'VARCHAR({size})'
    
    # Recherche avec DECIMAL(n,m)
    decimal_match = re.match(r'decimal\s*\(\s*(\d+)\s*,\s*(\d+)\s*\)', type_clean)
    if decimal_match:
        precision = int(decimal_match.group(1))
        scale = int(decimal_match.group(2))
        return f'DECIMAL({precision},{scale})'
    
    # Types contenant certains mots-cl√©s
    if any(keyword in type_clean for keyword in ['text', 'texte', 'long']):
        return 'TEXT'
    elif any(keyword in type_clean for keyword in ['int', 'entier', 'number']):
        return 'INTEGER'
    elif any(keyword in type_clean for keyword in ['float', 'decimal', 'double']):
        return 'DECIMAL(15,6)'
    elif any(keyword in type_clean for keyword in ['date', 'temps']):
        return 'DATE'
    elif any(keyword in type_clean for keyword in ['bool', 'vrai', 'faux']):
        return 'BOOLEAN'
    
    return None

def detect_column_type(clean_values: list, csv_separator: str = ';') -> str:
    """
    D√©tection universelle et intelligente du type SQL pour une colonne.
    Bas√©e uniquement sur l'analyse des donn√©es avec marges x8.
    
    Args:
        clean_values: Liste des valeurs nettoy√©es de la colonne
        csv_separator: S√©parateur CSV utilis√© (';' ou ',')
    
    Returns:
        Type SQL appropri√© avec marges x8 de s√©curit√©
    """
    if not clean_values:
        return 'VARCHAR(255)'
    
    # Analyse de base
    max_len = max(len(str(v)) for v in clean_values)
    
    # Test num√©rique ultra-strict
    all_numeric = True
    has_decimals = False
    
    for val in clean_values:
        val_str = str(val).strip()
        
        # Test num√©rique selon le s√©parateur
        if csv_separator == ';':
            # Format fran√ßais : virgule = d√©cimal
            if not re.match(r'^-?\d+(,\d*)?$', val_str.replace(' ', '')):
                all_numeric = False
                break
            if ',' in val_str:
                has_decimals = True
        else:
            # Format anglais : point = d√©cimal
            if not re.match(r'^-?\d+(\.\d*)?$', val_str.replace(' ', '')):
                all_numeric = False
                break
            if '.' in val_str:
                has_decimals = True
    
    # D√©cision finale
    if all_numeric:
        return 'DECIMAL(15,6)' if has_decimals else 'INTEGER'
    else:
        # VARCHAR avec marges x8 pures bas√©es uniquement sur les donn√©es
        if max_len <= 5:
            return 'VARCHAR(40)'    # Marge x8
        elif max_len <= 10:
            return 'VARCHAR(80)'    # Marge x8
        elif max_len <= 25:
            return 'VARCHAR(200)'   # Marge x8
        elif max_len <= 50:
            return 'VARCHAR(400)'   # Marge x8
        elif max_len <= 100:
            return 'VARCHAR(800)'   # Marge x8
        else:
            return 'TEXT'

def detect_column_type_with_column_name(clean_values: list, csv_separator: str, column_name: str) -> str:
    """
    D√©tection universelle et intelligente du type SQL avec prise en compte du nom de colonne.
    
    Args:
        clean_values: Liste des valeurs nettoy√©es de la colonne
        csv_separator: S√©parateur CSV utilis√© (';' ou ',')
        column_name: Nom de la colonne pour appliquer des r√®gles sp√©cifiques
    
    Returns:
        Type SQL appropri√© avec r√®gles intelligentes bas√©es sur le nom
    """
    col_lower = column_name.lower()
    
    # R√àGLE SP√âCIALE 1 : Colonnes de codes individuels ‚Üí VARCHAR(50)
    if ('code' in col_lower and 
        not col_lower.startswith('codes_') and  # Exclure codes_postaux
        not 'liste' in col_lower and           # Exclure autres listes
        not 'multiple' in col_lower):          # Exclure codes multiples
        return 'VARCHAR(50)'
    
    # R√àGLE SP√âCIALE 2 : Listes de codes ‚Üí VARCHAR(200) minimum
    if (col_lower.startswith('codes_') or 
        ('code' in col_lower and ('liste' in col_lower or 'multiple' in col_lower))):
        # Analyser les donn√©es pour voir si VARCHAR(200) suffit
        base_type = detect_column_type(clean_values, csv_separator)
        if base_type.startswith('VARCHAR'):
            try:
                detected_size = int(base_type.split('(')[1].split(')')[0])
                # Prendre le maximum entre 200 et la taille d√©tect√©e
                return f'VARCHAR({max(200, detected_size)})'
            except:
                return 'VARCHAR(200)'
        elif base_type == 'TEXT':
            return 'TEXT'
        else:
            return 'VARCHAR(200)'  # S√©curit√© par d√©faut
    
    # Sinon, utiliser la d√©tection normale bas√©e sur les donn√©es
    return detect_column_type(clean_values, csv_separator)

def parse_csv_line(line: str, separator: str) -> list:
    """Parse intelligente d'une ligne CSV avec gestion des guillemets."""
    import csv
    import io
    
    try:
        reader = csv.reader(io.StringIO(line), delimiter=separator, quotechar='"')
        return next(reader)
    except:
        return line.split(separator)

# Fonction de g√©n√©ration SQL SIMPLIFI√âE
def generate_sql_from_metadata(table_name: str) -> str:
    """G√©n√®re le script SQL d'import bas√© sur les m√©tadonn√©es."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM metadata WHERE nom_table = %s", (table_name,))
        result = cursor.fetchone()
        
        if not result:
            return f"‚ùå Table '{table_name}' non trouv√©e dans les m√©tadonn√©es"
        
        columns = [desc[0] for desc in cursor.description]
        metadata = dict(zip(columns, result))
        
        # Extraction des infos principales
        nom_table = metadata.get('nom_table', 'unknown_table')
        schema = metadata.get('schema', 'public')
        description = metadata.get('description', '')
        producteur = metadata.get('producteur', '')
        
        # R√©cup√©ration de la structure CSV
        contenu_csv = metadata.get('contenu_csv', {})
        if not contenu_csv or 'header' not in contenu_csv:
            return f"‚ùå Structure CSV non disponible pour '{table_name}'"
        
        colonnes = contenu_csv['header']
        separateur = contenu_csv.get('separator', ';')
        donnees_exemple = contenu_csv.get('data', [])
        
        # Affichage de debug
        st.write(f"üîç DEBUG: Colonnes trouv√©es: {len(colonnes)}")
        st.write(f"üîç DEBUG: Donn√©es d'exemple: {len(donnees_exemple)} lignes")
        
        # G√©n√©ration du SQL
        sql = f"""-- =====================================================================================
-- SCRIPT D'IMPORT POUR LA TABLE {nom_table}
-- =====================================================================================
-- Producteur: {producteur}
-- Schema: {schema}
-- Genere automatiquement le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- 
-- R√àGLES DE D√âTECTION DES TYPES :
-- 1. Priorit√© aux types d√©finis dans le dictionnaire des variables
-- 2. Codes individuels (code_insee, dep_code...) ‚Üí VARCHAR(50) (gestion ZZZZZZZZZ)
-- 3. Listes de codes (codes_postaux...) ‚Üí VARCHAR(200) minimum selon donn√©es
-- 4. Analyse des donn√©es avec marges de s√©curit√© x8
-- =====================================================================================

-- 1. Suppression de la table existante (si elle existe)
DROP TABLE IF EXISTS "{schema}"."{nom_table}";

-- 2. Creation de la table
CREATE TABLE "{schema}"."{nom_table}" (
"""
        
        # Traitement des colonnes avec debug
        cols = []
        for i, col in enumerate(colonnes):
            col_clean = col.strip()
            
            # R√©cup√©ration des valeurs d'exemple pour cette colonne
            sample_values = [row[i] if len(row) > i else None for row in donnees_exemple]
            clean_values = [str(v).strip() for v in sample_values if v is not None and str(v).strip()]
            
            # R√©cup√©ration du dictionnaire des variables s'il existe
            dictionnaire = metadata.get('dictionnaire', {})
            dict_data = dictionnaire.get('data', []) if dictionnaire else []
            
            # ==================================================================================
            # PRIORIT√â 1 : TYPES D√âFINIS PAR LE PRODUCTEUR DANS LE DICTIONNAIRE DES VARIABLES
            # ==================================================================================
            
            producteur_type = None
            if dict_data and len(dict_data) > 0:
                # Liste compl√®te des libell√©s possibles pour identifier une colonne de type
                type_column_labels = [
                    'type', 'data type', 'datatype', 'data_type', 
                    'field type', 'fieldtype', 'field_type',
                    'column type', 'column_type', 'columndatatype', 'column_data_type',
                    'valuetype', 'variable datatype', 'value_type', 'value_data_type', 
                    'variable_data_type', 'type de donn√©e', 'type de donn√©es', 'type_donnee'
                ]
                
                # M√âTHODE 1: Recherche par header du dictionnaire (si pr√©sent)
                dict_headers = dict_data[0] if len(dict_data) > 0 else []
                type_column_index = None
                data_start_index = 0  # Par d√©faut, pas de header
                
                # V√©rifier si la premi√®re ligne semble √™tre un header
                if dict_headers and len(dict_headers) >= 2:
                    first_row_seems_header = any(
                        str(header).strip().lower() in type_column_labels 
                        for header in dict_headers
                    )
                    
                    if first_row_seems_header:
                        data_start_index = 1  # Ignorer la premi√®re ligne (header)
                        # Chercher l'index de la colonne de type
                        for i, header in enumerate(dict_headers):
                            header_clean = str(header).strip().lower()
                            if header_clean in type_column_labels:
                                type_column_index = i
                                break
                
                # M√âTHODE 2: Recherche de la ligne correspondant √† notre colonne
                for dict_row in dict_data[data_start_index:]:  # Utiliser l'index appropri√©
                    if len(dict_row) >= 1 and dict_row[0].strip().lower() == col.lower():
                        # DEBUG: Afficher la ligne du dictionnaire pour cette colonne
                        if col_clean.lower() == 'dep_nom':
                            st.write(f"üîç DEBUG dep_nom - Ligne dictionnaire: {dict_row}")
                            st.write(f"üîç DEBUG dep_nom - Headers dictionnaire: {dict_headers}")
                            st.write(f"üîç DEBUG dep_nom - Index colonne type: {type_column_index}")
                        
                        # Si on a trouv√© une colonne de type d√©finie
                        if type_column_index is not None and len(dict_row) > type_column_index:
                            type_value = str(dict_row[type_column_index]).strip()
                            if type_value and type_value.lower() not in ['', 'nan', 'null', 'none']:
                                # Normalisation et validation du type
                                producteur_type = normalize_data_type(type_value)
                                if col_clean.lower() == 'dep_nom':
                                    st.write(f"üîç DEBUG dep_nom - Type brut trouv√©: '{type_value}' ‚Üí normalis√©: '{producteur_type}'")
                        
                        # M√âTHODE 3: Recherche dans toutes les colonnes si pas trouv√© via header
                        if not producteur_type:
                            for type_field in dict_row[1:]:  # √Ä partir de la 2√®me colonne
                                type_str = str(type_field).strip()
                                if type_str:
                                    normalized_type = normalize_data_type(type_str)
                                    if normalized_type:
                                        producteur_type = normalized_type
                                        if col_clean.lower() == 'dep_nom':
                                            st.write(f"üîç DEBUG dep_nom - Type trouv√© par scan: '{type_str}' ‚Üí '{producteur_type}'")
                                        break
                        break
            
            # ==================================================================================
            # PRIORIT√â 2 : ANALYSE CSV ULTRA-S√âCURIS√âE SI PAS DE TYPE DU PRODUCTEUR
            # ==================================================================================
            
            # DEBUG sp√©cial pour dep_nom
            if col_clean.lower() == 'dep_nom':
                st.write(f"üîç DEBUG dep_nom - √âchantillon CSV: {clean_values[:5]}")
                if clean_values:
                    max_len_found = max(len(str(v)) for v in clean_values)
                    st.write(f"üîç DEBUG dep_nom - Longueur max trouv√©e: {max_len_found}")
            
            # √âTAPE 1: Si le producteur a d√©fini un type, l'utiliser en priorit√© absolue
            if producteur_type:
                sql_type = producteur_type
                if col_clean.lower() == 'dep_nom':
                    st.write(f"üîç DEBUG dep_nom - UTILISATION TYPE PRODUCTEUR: {sql_type}")
            else:
                # √âTAPE 2: Analyse CSV avec r√®gles intelligentes (nom de colonne + donn√©es)
                sql_type = detect_column_type_with_column_name(clean_values, separateur, col_clean)
                
                # Debug pour les colonnes de codes
                col_lower = col_clean.lower()
                if ('code' in col_lower and 
                    not col_lower.startswith('codes_') and 
                    not 'liste' in col_lower and 
                    not 'multiple' in col_lower):
                    st.write(f"üîç DEBUG {col_clean} - Code individuel d√©tect√© ‚Üí VARCHAR(50)")
                elif (col_lower.startswith('codes_') or 
                      ('code' in col_lower and ('liste' in col_lower or 'multiple' in col_lower))):
                    st.write(f"üîç DEBUG {col_clean} - Liste de codes d√©tect√©e ‚Üí VARCHAR(200) minimum")
                
                if col_clean.lower() == 'dep_nom':
                    st.write(f"üîç DEBUG dep_nom - Type d√©tect√© final: {sql_type}")
            
            cols.append(f'    "{col_clean}" {sql_type}')
        
        sql += ",\n".join(cols)
        sql += "\n);\n"
        
        # Description
        if description:
            desc_lines = description.replace('\r\n', '\n').replace('\r', '\n').split('\n')
            sql += "\n-- ====================================================================================="
            sql += "\n-- DESCRIPTION DES DONNEES"
            sql += "\n-- ====================================================================================="
            for line in desc_lines:
                sql += f"\n-- {line}"
            sql += "\n-- ====================================================================================="
        
        conn.close()
        return sql
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la g√©n√©ration : {str(e)}")
        return f"‚ùå Erreur lors de la g√©n√©ration : {str(e)}"

# Configuration de la page
st.set_page_config(
    page_title="Saisie des m√©tadonn√©es",
    page_icon="üìù",
    layout="wide"
)

# Authentification centralis√©e (pr√©sente sur toutes les pages)
name, authentication_status, username, authenticator = authenticate_and_logout()

# Initialisation de la base de donn√©es
init_db()

# CSS pour le style du formulaire
st.markdown("""
<style>
    /* Style du titre */
    h1 {
        color: #1E4B88;
        font-size: 2rem;
        margin-bottom: 0.5rem;
    }
    
    /* Style des sous-titres */
    h2, h3, .stSubheader {
        color: #333;
        margin-top: 0;
        margin-bottom: 16px;
    }
    
    /* Style des bo√Ætes */
    .block-container {
        max-width: 1200px;
        padding-top: 1rem;
    }
    
    /* Style des champs obligatoires */
    .required label::after {
        content: " *";
        color: red;
    }
    
    /* Style du bouton de sauvegarde */
    .stButton button {
        background-color: #4CAF50;
        color: white;
        font-weight: 500;
        border-radius: 4px;
        padding: 0.5rem 1rem;
        width: 100%;
    }
    
    /* Am√©liorations visuelles g√©n√©rales */
    label {
        font-weight: 500;
    }
    
    .stInfo {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        border-left: 5px solid #1E88E5;
    }
    
    /* Style des messages de succ√®s */
    .stSuccess {
        background-color: #E8F5E9;
        padding: 16px;
        border-radius: 5px;
        border-left: 5px solid #4CAF50;
    }
    
    /* Style du s√©parateur */
    hr {
        margin-top: 2rem;
        margin-bottom: 2rem;
        border: 0;
        border-top: 1px solid #eee;
    }
</style>
""", unsafe_allow_html=True)

# Titre et description
st.title("Saisie des m√©tadonn√©es")
st.write("Remplissez le formulaire ci-dessous pour ajouter de nouvelles m√©tadonn√©es.")

# --- NOUVELLE ORGANISATION DU FORMULAIRE AVEC LOGIQUE DYNAMIQUE ---

# R√©cup√©ration des options depuis la base de donn√©es
from utils.db_utils import get_types_donnees, get_producteurs_by_type, get_jeux_donnees_by_producteur

# --- Affichage du formulaire restructur√© ---
col1, col2 = st.columns(2)

with col1:
    # Type de donn√©es
    types_donnees = get_types_donnees()
    type_sel = st.selectbox("Type de donn√©es*", types_donnees, key="type_donnees_select")
    if type_sel == "autre":
        type_donnees = st.text_input("Pr√©ciser le type de donn√©es*", key="type_donnees_input")
    else:
        type_donnees = type_sel

with col2:
    # Producteur de la donn√©e
    producteurs_dyn = get_producteurs_by_type(type_donnees)
    producteurs_options = producteurs_dyn + ["Autre"] if producteurs_dyn else ["Autre"]
    producteur_sel = st.selectbox("Producteur de la donn√©e*", producteurs_options, key="producteur_select")
    if producteur_sel == "Autre":
        producteur = st.text_input("Saisir un nouveau producteur*", key="producteur_input")
    else:
        producteur = producteur_sel

col3, col4 = st.columns(2)
with col3:
    # Nom du jeu de donn√©es
    jeux_dyn = get_jeux_donnees_by_producteur(producteur)
    jeux_options = jeux_dyn + ["Autre"] if jeux_dyn else ["Autre"]
    jeu_sel = st.selectbox("Nom du jeu de donn√©es*", jeux_options, key="jeu_donnees_select")
    if jeu_sel == "Autre":
        nom_jeu_donnees = st.text_input("Saisir un nouveau nom de jeu de donn√©es*", key="jeu_donnees_input")
    else:
        nom_jeu_donnees = jeu_sel

with col4:
    granularite_geo = st.selectbox("Granularit√© g√©ographique*", [
        "", "commune", "IRIS", "carreau", "adresse", "EPCI", "d√©partement", "r√©gion", "bassin de vie", "autre"
    ], index=1, help="Niveau g√©ographique le plus fin des donn√©es")

col5, col6 = st.columns(2)
with col5:
    date_publication = st.date_input("Date de publication*", 
        value=datetime.now().date(), 
        format="DD/MM/YYYY",
        min_value=datetime(1949, 1, 1).date())
with col6:
    date_maj = st.date_input("Date de derni√®re mise √† jour*", 
        value=datetime.now().date(), 
        format="DD/MM/YYYY",
        min_value=datetime(1949, 1, 1).date())
# S'assurer que date_maj est bien accessible partout
if 'date_maj' not in locals():
    date_maj = datetime.now().date()

col7, col8 = st.columns(2)
with col7:
    millesime = st.number_input("Mill√©sime/ann√©e*", min_value=1900, max_value=datetime.now().year, value=datetime.now().year, help="Ann√©e de r√©f√©rence des donn√©es")
with col8:
    frequence_maj = st.selectbox("Fr√©quence de mise √† jour des donn√©es*", [
        "", "Annuelle", "Semestrielle", "Trimestrielle", "Mensuelle", "Quotidienne", "Ponctuelle"
    ], help="Fr√©quence √† laquelle les donn√©es sont mises √† jour")
    def calculer_prochaine_publication(date_pub, freq):
        if not date_pub or not freq:
            return None
        if freq == "Annuelle":
            return date_pub + timedelta(days=365)
        elif freq == "Semestrielle":
            return date_pub + timedelta(days=182)
        elif freq == "Trimestrielle":
            return date_pub + timedelta(days=91)
        elif freq == "Mensuelle":
            return date_pub + timedelta(days=30)
        elif freq == "Quotidienne":
            return date_pub + timedelta(days=1)
        else:
            return None
    date_prochaine_publication_auto = calculer_prochaine_publication(date_publication, frequence_maj)
with col8:
    date_prochaine_publication = st.date_input(
        "Date estimative de la prochaine publication*",
        value=date_prochaine_publication_auto if date_prochaine_publication_auto else datetime.now().date(),
        format="DD/MM/YYYY",
        min_value=datetime(1949, 1, 1).date(),
        help="Cette date est calcul√©e automatiquement selon la fr√©quence, mais peut √™tre modifi√©e."
    )

# Description (pleine largeur)
st.markdown('<div class="required">', unsafe_allow_html=True)
description = st.text_area("Description*", height=150, help="Description d√©taill√©e des donn√©es, leur collecte, leur utilit√©, etc.", key="description")
st.markdown('</div>', unsafe_allow_html=True)

# --- Informations d'import ---
col9, col10 = st.columns(2)
with col9:
    nom_base = st.selectbox("Nom de la base de donn√©es*", ["opendata"], help="Nom de la base de donn√©es dans le SGBD")
with col10:
    schema = st.selectbox("Sch√©ma th√©matique*", [
        "economie", "education", "energie", "environnement", 
        "geo", "logement", "mobilite", "population", "reseau", "securite"
    ], help="Sch√©ma du SGBD dans lequel la table est import√©e")

# Nom de la table (moiti√© de ligne)
col11, _ = st.columns([1,1])
with col11:
    nom_table = st.text_input("Nom de la table*", help="Nom de la table dans la base de donn√©es")

# Note sur les champs obligatoires
st.markdown('<p style="color: #666;">Les champs marqu√©s d\'un * sont obligatoires.</p>', unsafe_allow_html=True)

# Section Informations suppl√©mentaires
st.subheader("Informations suppl√©mentaires")

col1, col2 = st.columns(2)

with col1:
    source = st.text_input("Source (URL)", help="Source des donn√©es (URL, nom du service, etc.)")
    
    licence = st.selectbox("Licence d'utilisation des donn√©es",
        ["", "Licence Ouverte Etalab", "Open Data Commons Open Database License (ODbL)", 
         "CC0", "CC BY", "CC BY-SA", "CC BY-NC", "CC BY-ND", 
         "Domaine public", "Licence propri√©taire"],
        help="Licence sous laquelle les donn√©es sont publi√©es")

with col2:
    envoi_par = st.text_input("Personne remplissant le formulaire", help="Nom de la personne remplissant ce formulaire")

# Sections d√©pliables
with st.expander("Extrait CSV", expanded=False):
    separateur = st.radio("S√©parateur", [";", ","], horizontal=True)
    contenu_csv = st.text_area("Coller ici les 50 premi√®res lignes du fichier CSV (incluant l'en-t√™te)", height=200, key="extrait_csv", 
                              help="Pour une meilleure d√©tection des types de donn√©es, copiez les 50 premi√®res lignes de votre fichier CSV. Plus d'exemples = meilleure pr√©cision du script SQL g√©n√©r√©.")

with st.expander("Dictionnaire des variables", expanded=False):
    dict_separateur = st.radio("S√©parateur du dictionnaire", [";", ","], horizontal=True)
    dictionnaire = st.text_area("Coller ici le dictionnaire des variables depuis le fichier CSV", height=150, key="dictionnaire")

# Boutons d'action
col_btn1, col_btn2 = st.columns(2)
with col_btn1:
submitted = st.button("Sauvegarder les m√©tadonn√©es")
with col_btn2:
    generate_sql = st.button("G√©n√©rer le script SQL d'import", help="G√©n√®re automatiquement le script SQL d'import bas√© sur les m√©tadonn√©es")

# Traitement de la soumission
if submitted:
    if not nom_table:
        st.error("Veuillez saisir un nom de table")
    else:
        try:
            # Pr√©paration du dictionnaire de m√©tadonn√©es avec les nouveaux champs √† la racine
            metadata = {
                "type_donnees": type_donnees,
                "nom_jeu_donnees": nom_jeu_donnees,
                "date_publication": date_publication.strftime("%Y-%m-%d") if date_publication else None,
                "date_maj": date_maj.strftime("%Y-%m-%d") if date_maj else None,
                "date_prochaine_publication": date_prochaine_publication.strftime("%Y-%m-%d") if date_prochaine_publication else None,
                "contenu_csv": {},
                "dictionnaire": {},
                "nom_fichier": nom_base,  # correspond √† nom_base dans la BD
                "nom_table": nom_table,
                "informations_base": {
                    "nom_table": nom_table,
                    "nom_base": nom_base,
                    "type_donnees": type_donnees,
                    "producteur": producteur,
                    "nom_jeu_donnees": nom_jeu_donnees,
                    "schema": schema,
                    "description": description,
                    "date_creation": str(millesime),
                    "source": source,
                    "frequence_maj": frequence_maj,
                    "licence": licence,
                    "envoi_par": envoi_par,
                    "separateur_csv": separateur,
                    "granularite_geo": granularite_geo
                }
            }
            
            # Traitement du contenu CSV
            if contenu_csv:
                try:
                    lines = contenu_csv.strip().split('\n')
                    
                    # Parser l'en-t√™te avec la fonction unifi√©e
                    header = parse_csv_line(lines[0], separateur)
                    
                    # Parser les donn√©es (maximum 50 lignes pour performance)
                    data_rows = []
                    for line in lines[1:51]:  # Limiter √† 50 lignes de donn√©es
                        if line.strip():  # Ignorer les lignes vides
                            parsed_row = parse_csv_line(line, separateur)
                            data_rows.append(parsed_row)
                    
                    metadata["contenu_csv"] = {
                        "header": header,
                        "data": data_rows,
                        "separator": separateur
                    }
                except Exception as e:
                    st.warning(f"Erreur lors de l'analyse du CSV : {str(e)}")
                    st.warning("V√©rifiez le format CSV et le s√©parateur choisi.")
            
            # Traitement du dictionnaire des variables
            if dictionnaire:
                try:
                    lines = dictionnaire.strip().split('\n')
                    
                    # S'assurer qu'il y a au moins une ligne d'en-t√™te
                    if not lines:
                        st.warning("Le dictionnaire est vide, il sera ignor√©.")
                    else:
                        # Parser l'en-t√™te avec la fonction unifi√©e
                        header = parse_csv_line(lines[0], dict_separateur)
                        
                        # Parser les donn√©es avec la fonction unifi√©e
                        data_rows = []
                        for line in lines[1:]:
                            if line.strip():  # Ignorer les lignes vides
                                parsed_row = parse_csv_line(line, dict_separateur)
                                data_rows.append(parsed_row)
                        
                        # V√©rifier si nous avons des donn√©es
                        if not data_rows:
                            st.warning("Le dictionnaire ne contient pas de donn√©es, uniquement l'en-t√™te.")
                        
                        # Uniformiser les donn√©es pour √©viter les erreurs
                        uniform_data = []
                        for row in data_rows:
                            # Si la ligne a moins de colonnes que l'en-t√™te, ajouter des valeurs vides
                            if len(row) < len(header):
                                row.extend([''] * (len(header) - len(row)))
                            # Si la ligne a plus de colonnes que l'en-t√™te, tronquer
                            elif len(row) > len(header):
                                row = row[:len(header)]
                            uniform_data.append(row)
                        
                        metadata["dictionnaire"] = {
                            "header": header,
                            "data": uniform_data[:2000],  # Limiter √† 2000 lignes maximum
                            "separator": dict_separateur
                        }
                except Exception as e:
                    st.warning(f"Erreur lors de l'analyse du dictionnaire : {str(e)}")
                    st.warning("Le dictionnaire sera ignor√©.")
                    metadata["dictionnaire"] = {}

            # Sauvegarde dans la base de donn√©es
            succes, message = save_metadata(metadata)
            if succes:
                st.balloons()
                st.success("""
### ‚úÖ M√©tadonn√©es sauvegard√©es avec succ√®s!

Vos m√©tadonn√©es sont maintenant disponibles dans le catalogue.
Vous pouvez les consulter dans l'onglet "Catalogue".
""")
            else:
                st.error(f"Erreur lors de la sauvegarde : {message}")
                st.error("Veuillez v√©rifier les logs pour plus de d√©tails.")

            # Sauvegarde locale en JSON
            try:
                json_path = os.path.join("metadata", f"{nom_table}.json")
                os.makedirs("metadata", exist_ok=True)
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=4)
            except Exception as e:
                st.error(f"Erreur lors de la sauvegarde locale en JSON : {str(e)}")

            # Sauvegarde locale en TXT
            try:
                txt_path = os.path.join("metadata", f"{nom_table}.txt")
                with open(txt_path, "w", encoding="utf-8") as f:
                    f.write(f"Nom de la table : {nom_table}\n")
                    f.write(f"Nom de la base de donn√©es : {nom_base}\n")
                    f.write(f"Sch√©ma du SGBD : {schema}\n")
                    f.write(f"Producteur de la donn√©e : {producteur}\n")
                    f.write(f"Granularit√© g√©ographique : {granularite_geo}\n")
                    f.write(f"Description : {description}\n")
                    f.write(f"Mill√©sime/ann√©e : {millesime}\n")
                    f.write(f"Source : {source}\n")
                    f.write(f"Fr√©quence de mises √† jour des donn√©es : {frequence_maj}\n")
                    f.write(f"Licence d'utilisation des donn√©es : {licence}\n")
                    f.write(f"Personne remplissant le formulaire : {envoi_par}\n")
                    f.write(f"S√©parateur CSV : {separateur}\n")
                    if contenu_csv:
                        f.write("\nContenu CSV :\n")
                        f.write(contenu_csv)
                    if dictionnaire:
                        f.write("\nDictionnaire des variables :\n")
                        f.write(dictionnaire)
            except Exception as e:
                st.error(f"Erreur lors de la sauvegarde locale en TXT : {str(e)}")
                
        except Exception as e:
            st.error(f"Erreur inattendue : {str(e)}")
            st.error("Veuillez v√©rifier les logs pour plus de d√©tails.")

# Traitement du bouton de g√©n√©ration SQL
if generate_sql:
    if not nom_table:
        st.error("Veuillez d'abord saisir un nom de table pour g√©n√©rer le script SQL")
    else:
        with st.spinner("G√©n√©ration du script SQL en cours..."):
            sql_script = generate_sql_from_metadata(nom_table)
            
            if sql_script.startswith("‚ùå"):
                st.error(sql_script)
            else:
                st.success("üéâ Script SQL g√©n√©r√© avec succ√®s !")
                
                # Affichage du script avec possibilit√© de copier
                st.subheader("üìÑ Script SQL d'import g√©n√©r√©")
                st.code(sql_script, language="sql")
                
                # Bouton de t√©l√©chargement
                st.download_button(
                    label="üíæ T√©l√©charger le script SQL",
                    data=sql_script,
                    file_name=f"import_{nom_table}.sql",
                    mime="text/plain"
                )
                
                st.info("""
                ### üìã Instructions d'utilisation :
                1. **T√©l√©chargez** le script SQL ci-dessus
                2. **Modifiez** le chemin du fichier CSV dans la section COPY
                3. **Ex√©cutez** le script dans votre outil de gestion PostgreSQL (DBeaver, pgAdmin, etc.)
                """)

# Section d'aide
with st.expander("Aide pour la saisie ‚ùì"):
    st.markdown("""
    ### Champs obligatoires
    Les champs marqu√©s d'un ast√©risque (*) sont obligatoires.
    
    - **Sch√©ma du SGBD** : Sch√©ma de la table dans le SGBD Intelligence des Territoires
    - **Nom de la table** : Nom de la table dans le SGBD Intelligence des Territoires
    - **Table de la base de donn√©es** : Nom de la table dans le SGBD Intelligence des Territoires
    - **Producteur de la donn√©e** : Nom de l'organisme pourvoyeur de la donn√©e
    - **Mill√©sime/ann√©e** : Ann√©e de r√©f√©rence des donn√©es
    - **Derni√®re mise √† jour** : Date de la derni√®re mise √† jour des donn√©es
    - **Description** : Description d√©taill√©e du contenu des donn√©es
    
    ### Conseils de saisie
    1. **Informations de base**
       - Le nom de la table est actuellement limit√© √† certaines valeurs pr√©d√©finies
       - Le sch√©ma doit correspondre √† l'une des cat√©gories pr√©d√©finies
       - Le producteur doit √™tre l'organisme source des donn√©es
       - Le mill√©sime doit correspondre √† l'ann√©e de r√©f√©rence des donn√©es
    
    2. **Description**
       - Soyez aussi pr√©cis que possible dans la description
    
    3. **Informations suppl√©mentaires**
       - Indiquez la personne qui remplit le formulaire
       - Pr√©cisez la source originale des donn√©es (URL ou r√©f√©rence)
       - S√©lectionnez la fr√©quence de mise √† jour appropri√©e
       - Choisissez la licence qui correspond aux conditions d'utilisation
    
    4. **Donn√©es CSV**
       - Copiez-collez les 50 premi√®res lignes de votre fichier CSV (en-t√™te inclus)
       - Plus d'exemples = meilleure pr√©cision du script SQL g√©n√©r√© automatiquement
       - Indiquez le s√©parateur utilis√© (point-virgule par d√©faut pour les fichiers fran√ßais)
       - Ajoutez le dictionnaire des variables si disponible pour optimiser la d√©tection des types
    """)

# Pied de page
st.markdown("---")
st.markdown('<div style="text-align: center; color: #666;">¬© 2025 - Syst√®me de Gestion des M√©tadonn√©es</div>', unsafe_allow_html=True) 