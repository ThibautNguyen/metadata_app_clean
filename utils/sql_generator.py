"""
Module de g√©n√©ration automatique de scripts SQL d'import
Contient toutes les fonctions n√©cessaires pour analyser les m√©tadonn√©es
et g√©n√©rer des scripts SQL PostgreSQL optimis√©s.
"""

import streamlit as st
import pandas as pd
import re
import io
import csv
from datetime import datetime
from typing import List, Optional, Tuple, Dict
from .db_utils import get_db_connection


def normalize_data_type(raw_type: str) -> str:
    """
    Normalise un type de donn√©es brut vers un type SQL standard.
    
    Args:
        raw_type: Type brut trouv√© dans le dictionnaire des variables
        
    Returns:
        Type SQL standardis√© ou None si non reconnu
    """
    if not raw_type:
        return None
    
    type_clean = str(raw_type).strip().lower()
    
    # Mapping des types courants vers SQL
    type_mappings = {
        # Types texte
        'text': 'TEXT',
        'string': 'VARCHAR(255)',
        'str': 'VARCHAR(255)', 
        'char': 'VARCHAR(255)',
        'character': 'VARCHAR(255)',
        'varchar': 'VARCHAR(255)',
        'texte': 'TEXT',
        'cha√Æne': 'VARCHAR(255)',
        'chaine': 'VARCHAR(255)',
        
        # Types num√©riques entiers
        'int': 'INTEGER',
        'integer': 'INTEGER',
        'entier': 'INTEGER',
        'number': 'INTEGER',
        'numeric': 'INTEGER',
        'num': 'INTEGER',
        'bigint': 'BIGINT',
        'smallint': 'SMALLINT',
        
        # Types num√©riques d√©cimaux
        'float': 'DECIMAL(15,6)',
        'decimal': 'DECIMAL(15,6)',
        'double': 'DECIMAL(15,6)',
        'real': 'DECIMAL(15,6)',
        'd√©cimal': 'DECIMAL(15,6)',
        'flottant': 'DECIMAL(15,6)',
        
        # Types date/temps
        'date': 'DATE',
        'datetime': 'TIMESTAMP',
        'timestamp': 'TIMESTAMP',
        'time': 'TIME',
        # CORRECTION : Retirer 'temps' g√©n√©rique pour √©viter "temps partiel" ‚Üí DATE
        # 'temps': 'TIMESTAMP',  # Trop g√©n√©rique, comment√©
        
        # Types bool√©ens
        'boolean': 'BOOLEAN',
        'bool': 'BOOLEAN',
        'bool√©en': 'BOOLEAN',
        'vrai/faux': 'BOOLEAN',
        'oui/non': 'BOOLEAN',
        
        # Types binaires/blob
        'blob': 'BYTEA',
        'binary': 'BYTEA',
        'binaire': 'BYTEA'
    }
    
    # Recherche directe
    if type_clean in type_mappings:
        return type_mappings[type_clean]
    
    # Recherche avec VARCHAR(n)
    varchar_match = re.match(r'varchar\s*\(\s*(\d+)\s*\)', type_clean)
    if varchar_match:
        size = int(varchar_match.group(1))
        return f'VARCHAR({size})'
    
    # Recherche avec DECIMAL(n,m)
    decimal_match = re.match(r'decimal\s*\(\s*(\d+)\s*,\s*(\d+)\s*\)', type_clean)
    if decimal_match:
        precision = int(decimal_match.group(1))
        scale = int(decimal_match.group(2))
        return f'DECIMAL({precision},{scale})'
    
    # Types contenant certains mots-cl√©s (patterns PR√âCIS pour √©viter les faux positifs)
    if any(keyword in type_clean for keyword in ['text', 'texte', 'long']):
        return 'TEXT'
    elif any(keyword in type_clean for keyword in ['int', 'entier', 'number']):
        return 'INTEGER'
    elif any(keyword in type_clean for keyword in ['float', 'decimal', 'double']):
        return 'DECIMAL(15,6)'
    elif any(keyword in type_clean for keyword in ['date', 'datetime', 'timestamp']):
        # CORRECTION : Retirer 'temps' g√©n√©rique qui capture "temps partiel"
        # √ätre plus sp√©cifique pour √©viter les faux positifs
        return 'DATE'
    elif any(keyword in type_clean for keyword in ['bool', 'vrai', 'faux']):
        return 'BOOLEAN'
    
    return None


def _detect_column_type_from_dictionary(dict_info: Dict) -> Optional[str]:
    """
    Analyse intelligente du dictionnaire pour d√©tecter le type de donn√©es.
    Recherche des patterns dans toutes les colonnes du dictionnaire.
    """
    # Patterns pour la d√©tection des types
    type_patterns = {
        'codes': [
            r'code[s]?\s*(?:possible|autoris√©|valide|d√©taill√©)',
            r'modalit√©[s]?\s*(?:possible|autoris√©e)',
            r'correspondance[s]?[\s_](?:code|valeur)',
            r'valeur[s]?\s*(?:possible|autoris√©e)',
            r'liste\s*(?:des\s*)?(?:code|valeur)',
            r'nomenclature',
            r'codification',
        ],
        'text_type': [
            r'type[s]?[\s_](?:donn√©e|variable|champ)',
            r'format[\s_](?:donn√©e|variable|champ)',
            r'nature[\s_](?:donn√©e|variable|champ)',
            r'caract√©ristique[\s_](?:donn√©e|variable)',
            r'description[\s_]type',
        ]
    }

    # Fonction pour d√©tecter les patterns dans une cha√Æne
    def has_pattern(text: str, patterns: List[str]) -> bool:
        if not isinstance(text, str):
            return False
        text = text.lower()
        return any(re.search(pattern.lower(), text) for pattern in patterns)

    # Parcourir toutes les cl√©s du dictionnaire
    for key, value in dict_info.items():
        key_lower = key.lower()

        # 1. Recherche de colonnes contenant des codes/modalit√©s
        if has_pattern(key_lower, type_patterns['codes']):
            if isinstance(value, str):
                # Extraction des codes num√©riques (en g√©rant les n√©gatifs)
                numeric_codes = re.findall(r'[-]?\d+', value)
                if numeric_codes:
                    # Si tous les codes sont num√©riques
                    if all(c.strip('-').isdigit() for c in numeric_codes):
                        max_val = max(abs(int(c)) for c in numeric_codes)
                        if max_val < 128:
                            return 'SMALLINT'
                        return 'INTEGER'
                    
                # Si les codes sont alphanum√©riques, d√©terminer la longueur max
                all_codes = re.findall(r'[A-Za-z0-9]+', value)
                if all_codes:
                    max_length = max(len(code) for code in all_codes)
                    return f'VARCHAR({max(max_length * 2, 20)})'  # Marge de s√©curit√©

        # 2. Recherche de colonnes d√©crivant le type
        if has_pattern(key_lower, type_patterns['text_type']):
            if isinstance(value, str):
                value_lower = value.lower()
                
                # D√©tection du type √† partir de la description
                if any(x in value_lower for x in ['entier', 'integer', 'nombre entier']):
                    return 'INTEGER'
                elif any(x in value_lower for x in ['decimal', 'r√©el', 'float', 'nombre d√©cimal']):
                    return 'DECIMAL(10,3)'
                elif any(x in value_lower for x in ['bool√©en', 'boolean', 'vrai/faux', 'oui/non']):
                    return 'BOOLEAN'
                elif any(x in value_lower for x in ['date', 'datetime']):
                    return 'DATE'
                elif 'texte' in value_lower or 'caract√®re' in value_lower:
                    # Recherche d'une longueur sp√©cifi√©e
                    length_match = re.search(r'(\d+)[\s]*(?:caract√®re|char)', value_lower)
                    if length_match:
                        return f'VARCHAR({length_match.group(1)})'
                    return 'VARCHAR(255)'

    return None


def detect_column_type(clean_values: list, csv_separator: str = ';', column_name: str = '', dict_info: Dict = None) -> str:
    """
    D√©tection universelle et intelligente du type SQL pour une colonne.
    Bas√©e sur l'analyse des donn√©es, du nom de colonne et du dictionnaire.
    
    Args:
        clean_values: Liste des valeurs nettoy√©es de la colonne
        csv_separator: S√©parateur CSV utilis√© (';' ou ',')
        column_name: Nom de la colonne pour une meilleure d√©tection
        dict_info: Informations du dictionnaire pour cette colonne
    
    Returns:
        Type SQL appropri√© avec marges de s√©curit√©
    """
    if not clean_values:
        return 'VARCHAR(255)'
    
    # 1. V√©rification du dictionnaire si disponible
    if dict_info:
        dict_type = _detect_column_type_from_dictionary(dict_info)
        if dict_type:
            return dict_type
    
    # 2. Analyse du nom de colonne
    col_lower = column_name.lower() if column_name else ''
    
    # D√©tection des colonnes g√©ographiques
    if any(col_lower == x for x in ['lat', 'latitude', 'long', 'longitude']):
        return 'DECIMAL(10,6)'
    
    # D√©tection des codes g√©ographiques
    if col_lower in ['codgeo', 'code_insee', 'insee']:
        return 'VARCHAR(5)'
    elif any(pattern in col_lower for pattern in ['code_dep', 'dep']):
        return 'VARCHAR(3)'
    elif any(pattern in col_lower for pattern in ['code_reg', 'reg']):
        return 'VARCHAR(2)'
    elif col_lower.startswith('code') or col_lower.endswith('_id'):
        return 'VARCHAR(20)'
    
    # D√©tection des dates et ann√©es
    elif any(pattern in col_lower for pattern in ['annee', 'year']):
        return 'INTEGER'
    elif any(pattern in col_lower for pattern in ['date', 'timestamp']):
        return 'DATE'
    
    # D√©tection des pourcentages
    elif any(pattern in col_lower for pattern in ['taux', '%', 'pct', 'pourcentage', 'part']):
        return 'DECIMAL(5,2)'
    
    # 3. Analyse des valeurs avec gestion du masquage INSEE
    max_len = max(len(str(v)) for v in clean_values)
    
    # D√©tection du masquage INSEE
    insee_masking_patterns = ['ZZZZZZ', 'ZZZZZ', 'ZZZZ', 'ZZZ', 'XX', 'XXX', 'XXXX', 's', 'SECRET']
    has_insee_masking = any(
        str(val).strip().upper() in insee_masking_patterns 
        for val in clean_values
    )
    
    if has_insee_masking:
        if max_len <= 10:
            return 'VARCHAR(50)'
        elif max_len <= 25:
            return 'VARCHAR(200)'   
        else:
            return 'TEXT'
    
    # Test num√©rique strict
    all_numeric = True
    has_decimals = False
    
    for val in clean_values:
        val_str = str(val).strip()
        
        # Gestion selon le s√©parateur d√©cimal
        if csv_separator == ';':
            # Format fran√ßais
            if not re.match(r'^-?\d+(,\d*)?$', val_str.replace(' ', '')):
                all_numeric = False
                break
            if ',' in val_str:
                has_decimals = True
        else:
            # Format anglais
            if not re.match(r'^-?\d+(\.\d*)?$', val_str.replace(' ', '')):
                all_numeric = False
                break
            if '.' in val_str:
                has_decimals = True
    
    # D√©cision finale avec marges de s√©curit√©
    if all_numeric:
        if has_decimals:
            return 'DECIMAL(15,6)'
        else:
            # Test pour les petits entiers
            try:
                max_val = max(abs(int(str(v).replace(' ', ''))) for v in clean_values)
                if max_val < 32768:
                    return 'SMALLINT'
                elif max_val < 2147483648:
                    return 'INTEGER'
                else:
                    return 'BIGINT'
            except:
                return 'INTEGER'
    else:
        # VARCHAR avec marges de s√©curit√©
        if max_len <= 5:
            return 'VARCHAR(40)'
        elif max_len <= 10:
            return 'VARCHAR(80)'
        elif max_len <= 25:
            return 'VARCHAR(200)'
        elif max_len <= 50:
            return 'VARCHAR(400)'
        elif max_len <= 100:
            return 'VARCHAR(800)'
        else:
            return 'TEXT'


def detect_type_from_description(description: str) -> str:
    """
    D√©tection universelle du type SQL bas√©e sur l'analyse s√©mantique des descriptions.
    Cette fonction exploite les descriptions pour identifier les codes/identifiants (protection anti-ZZZZZZ).
    
    Args:
        description: Description de la variable (libell√© + description longue)
        
    Returns:
        Type SQL sugg√©r√© ou None si aucune d√©tection claire
    """
    if not description:
        return None
        
    desc_lower = description.lower()
    
    # R√àGLE 1 PRIORITAIRE: Comptages ‚Üí INTEGER (plus sp√©cifique, doit venir AVANT les codes)
    counting_keywords = [
        'nombre de', "nombre d'", 'count', 'total', 'effectif', 'population', 
        'nb de', 'quantit√©', 'quantite', 'effectifs'
    ]
    if any(keyword in desc_lower for keyword in counting_keywords):
        return 'INTEGER'
    
    # R√àGLE 2: Taux/Pourcentages/Ratios ‚Üí DECIMAL (avant les codes aussi)
    ratio_keywords = [
        'taux', 'pourcentage', 'ratio', 'rate', 'part de', 'proportion',
        'indice', 'moyenne', 'median', 'percentile'
    ]
    if any(keyword in desc_lower for keyword in ratio_keywords):
        return 'DECIMAL(15,6)'
    
    # R√àGLE 3: Codes/Identifiants ‚Üí VARCHAR (protection universelle anti-ZZZZZZ)
    # Attention : exclure "aide"/"aid√©" qui ne sont pas des codes mais des cat√©gories
    code_keywords = [
        'code', 'codes', 'identifiant', 'identifier', 'num√©ro', 'numero', 
        'r√©f√©rence', 'reference', 'cl√©', 'cle', 'key', 'id', 'siren', 'siret'
    ]
    # V√©rification plus stricte : ne pas confondre "aide" avec "code"
    if any(keyword in desc_lower for keyword in code_keywords):
        # Exclure les faux positifs comme "emplois aid√©s", "aides familiaux"
        false_positives = ['emplois aid√©s', 'aides familiaux', 'emploi aid√©', 'aide familial']
        if not any(fp in desc_lower for fp in false_positives):
            return 'VARCHAR(50)'
    
    # R√àGLE 4: Libell√©s/Noms ‚Üí diff√©rentes tailles selon le type
    
    # R√àGLE 4a: Libell√©s g√©ographiques sp√©cifiques ‚Üí VARCHAR(200)
    geo_label_keywords = [
        'libell√© de la commune', 'libell√© commune', 'libell√© de l\'iris', 'libell√© iris',
        'label de l\'iris', 'label iris',  # Ajout pour LAB_IRIS
        'libcom', 'libiris', 'lab_iris'
    ]
    if any(keyword in desc_lower for keyword in geo_label_keywords):
        return 'VARCHAR(200)'  # Pour les libell√©s g√©ographiques (peuvent √™tre longs)
    
    # R√àGLE 4b: Autres libell√©s ‚Üí VARCHAR(255)
    text_keywords = [
        'libell√©', 'libelle', 'nom de', 'intitul√©', 'intitule', 
        'designation', 'd√©signation', 'appellation'
    ]
    if any(keyword in desc_lower for keyword in text_keywords):
        return 'VARCHAR(255)'  # Pour les libell√©s courts/moyens standards
    
    return None  # Aucune d√©tection s√©mantique claire


def detect_column_type_with_column_name(clean_values: list, csv_separator: str, column_name: str) -> str:
    """
    D√©tection universelle et intelligente du type SQL avec prise en compte du nom de colonne.
    Utilise des patterns PR√âCIS pour √©viter les faux positifs.
    
    Args:
        clean_values: Liste des valeurs nettoy√©es de la colonne
        csv_separator: S√©parateur CSV utilis√© (';' ou ',')
        column_name: Nom de la colonne pour appliquer des r√®gles sp√©cifiques
    
    Returns:
        Type SQL appropri√© avec r√®gles intelligentes bas√©es sur le nom
    """
    col_lower = column_name.lower()
    
    # R√àGLE PRIORITAIRE NOUVELLE : Libell√©s g√©ographiques ‚Üí VARCHAR(200)
    label_patterns = [
        r'^libcom$', r'^libiris$', r'^lab_iris$'  # Vrais libell√©s descriptifs
    ]
    
    # Si le nom de colonne matche un pattern de libell√© ‚Üí VARCHAR(200)
    if any(re.search(pattern, col_lower) for pattern in label_patterns):
        return 'VARCHAR(200)'
    
    # R√àGLE UNIVERSELLE 1 : D√©tection PR√âCISE des identifiants et codes
    # Utilisation de regex pour des patterns exacts et √©viter les faux positifs
    identifier_patterns = [
        # Codes explicites (patterns exacts ou d√©limit√©s)
        r'^code$', r'^cod$', r'^id$', r'_id$', r'^id_', r'_code$', r'^code_',
        r'^identifier$', r'^identifiant$',
        # Codes g√©ographiques INSEE (patterns pr√©cis uniquement)
        r'^iris$', r'^triris$', r'^codgeo$', r'^geocode$', 
        r'^commune$', r'^com$', r'^dep$', r'^reg$', r'^uu\d*$',
        # Codes d'entreprises (patterns pr√©cis)
        r'^siren$', r'^siret$', r'^nic$', r'^ape$', r'^naf$', r'^tva$',
        # Autres identifiants (patterns d√©limit√©s)
        r'^reference$', r'^ref$', r'^numero$', r'^num$', r'^matricule$', 
        r'^cle$', r'^key$', r'_ref$', r'_key$',
        # Indicateurs techniques g√©ographiques (SANS les vrais libell√©s)
        r'^typ_iris$', r'^modif_iris$',
    ]
    
    # Si le nom de colonne matche un pattern d'identifiant PR√âCIS ‚Üí VARCHAR(50)
    if any(re.search(pattern, col_lower) for pattern in identifier_patterns):
        return 'VARCHAR(50)'
    
    # R√àGLE UNIVERSELLE 2 : Listes de codes ‚Üí VARCHAR(200) minimum
    if (col_lower.startswith('codes_') or 
        ('code' in col_lower and ('liste' in col_lower or 'multiple' in col_lower))):
        # Analyser les donn√©es pour voir si VARCHAR(200) suffit
        base_type = detect_column_type(clean_values, csv_separator)
        if base_type.startswith('VARCHAR'):
            try:
                detected_size = int(base_type.split('(')[1].split(')')[0])
                # Prendre le maximum entre 200 et la taille d√©tect√©e
                return f'VARCHAR({max(200, detected_size)})'
            except:
                return 'VARCHAR(200)'
        elif base_type == 'TEXT':
            return 'TEXT'
        else:
            return 'VARCHAR(200)'  # S√©curit√© par d√©faut
    
    # Sinon, utiliser la d√©tection normale bas√©e sur les donn√©es
    return detect_column_type(clean_values, csv_separator)


def detect_column_type_intelligent_universal(clean_values: list, csv_separator: str, column_name: str, dict_row: list = None) -> str:
    """
    D√©tection universelle et intelligente selon la hi√©rarchie de priorit√©s d√©finie.
    
    HI√âRARCHIE DES PRIORIT√âS :
    1. Type explicite dans le dictionnaire des variables
    2. Analyse s√©mantique des descriptions (protection anti-ZZZZZZ universelle)
    3. Analyse des donn√©es r√©elles
    4. Patterns de noms de colonnes (fallback uniquement)
    
    Args:
        clean_values: Liste des valeurs nettoy√©es de la colonne
        csv_separator: S√©parateur CSV utilis√©
        column_name: Nom de la colonne
        dict_row: Ligne du dictionnaire des variables [nom, libelle, description, ...]
        
    Returns:
        Type SQL appropri√© selon la hi√©rarchie
    """
    
    # PROTECTION G√âOGRAPHIQUE ABSOLUE : Identifiants g√©ographiques critiques
    # Ces colonnes DOIVENT √™tre VARCHAR m√™me si les donn√©es semblent num√©riques (protection anti-ZZZZZZ)
    col_lower = column_name.lower()
    
    # Protection sp√©ciale pour les libell√©s g√©ographiques ‚Üí VARCHAR(200)
    geo_label_patterns = [r'^libcom$', r'^libiris$', r'^lab_iris$']
    if any(re.search(pattern, col_lower) for pattern in geo_label_patterns):
        return 'VARCHAR(200)'  # Protection absolue pour les libell√©s g√©ographiques
    
    # Protection pour les identifiants techniques ‚Üí VARCHAR(50)
    geographic_critical_patterns = [
        r'^iris$', r'^triris$', r'^codgeo$', r'^geocode$', 
        r'^commune$', r'^com$', r'^dep$', r'^reg$', r'^uu\d*$',
        r'^typ_iris$', r'^modif_iris$'  # Seulement les identifiants techniques
    ]
    if any(re.search(pattern, col_lower) for pattern in geographic_critical_patterns):
        return 'VARCHAR(50)'  # Protection absolue pour les codes seulement
    
    # PRIORIT√â 1 : Type explicite dans le dictionnaire des variables
    if dict_row and len(dict_row) > 0:
        # Rechercher un type explicite dans les colonnes du dictionnaire
        for field in dict_row[1:]:  # √Ä partir de la 2√®me colonne
            field_str = str(field).strip()
            if field_str:
                explicit_type = normalize_data_type(field_str)
                if explicit_type:
                    return explicit_type
    
    # PRIORIT√â 2 : Analyse s√©mantique des descriptions (protection universelle anti-ZZZZZZ)
    if dict_row and len(dict_row) > 1:
        # Concat√©ner libell√© + description pour l'analyse s√©mantique
        description_complete = ' '.join(str(field) for field in dict_row[1:])
        semantic_type = detect_type_from_description(description_complete)
        if semantic_type:
            # Protection sp√©ciale : Si l'analyse s√©mantique d√©tecte VARCHAR(200) pour un libell√© g√©ographique,
            # forcer ce type (priorit√© absolue sur l'analyse des donn√©es)
            if semantic_type == 'VARCHAR(200)':
                return semantic_type
            return semantic_type
    
    # PRIORIT√â 3 : Analyse des donn√©es r√©elles
    data_based_type = detect_column_type(clean_values, csv_separator)
    
    # PRIORIT√â 4 : Affiner avec patterns de noms seulement si n√©cessaire
    # (uniquement pour les cas ambigus de VARCHAR)
    if data_based_type.startswith('VARCHAR') and len(clean_values) > 0:
        # V√©rifier si les patterns de noms peuvent affiner le type
        name_based_type = detect_column_type_with_column_name(clean_values, csv_separator, column_name)
        
        # Si le pattern de nom sugg√®re un type plus pr√©cis, l'utiliser
        if name_based_type != data_based_type and name_based_type in ['VARCHAR(50)', 'VARCHAR(200)']:
            return name_based_type
    
    return data_based_type


def parse_csv_line(line: str, separator: str) -> list:
    """Parse intelligente d'une ligne CSV avec gestion des guillemets."""
    try:
        reader = csv.reader(io.StringIO(line), delimiter=separator, quotechar='"')
        return next(reader)
    except:
        return line.split(separator)


def generate_sql_from_metadata(table_name: str, debug_mode: bool = False) -> str:
    """
    G√©n√®re une requ√™te SQL d'import compl√®te bas√©e sur les m√©tadonn√©es stock√©es.
    Version am√©lior√©e avec support complet du dictionnaire et d√©tection intelligente des types.
    
    Args:
        table_name: Nom de la table dans la base de m√©tadonn√©es
        debug_mode: Si True, affiche des informations de d√©bogage
    
    Returns:
        Script SQL complet pour l'import des donn√©es
    """
    try:
        # Connexion √† la base de m√©tadonn√©es
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©ration des m√©tadonn√©es
        cursor.execute("SELECT * FROM metadata WHERE nom_table = %s", (table_name,))
        result = cursor.fetchone()
        
        if not result:
            raise ValueError(f"Table '{table_name}' non trouv√©e dans la base de m√©tadonn√©es")
        
        # Conversion en dictionnaire
        columns = [desc[0] for desc in cursor.description]
        metadata = dict(zip(columns, result))
        
        # Extraction des informations principales
        nom_table = metadata.get('nom_table', 'unknown_table')
        schema = metadata.get('schema', 'public')
        nom_base = metadata.get('nom_base', 'database')
        description = metadata.get('description', '')
        producteur = metadata.get('producteur', '')
        type_donnees = metadata.get('type_donnees', '')
        date_maj = metadata.get('date_maj', '')
        frequence_maj = metadata.get('frequence_maj', '')
        millesime = metadata.get('date_creation', '')
        
        # Extraction du contenu CSV et du dictionnaire
        contenu_csv = metadata.get('contenu_csv', {})
        dictionnaire = metadata.get('dictionnaire', {})
        
        # V√©rification de la pr√©sence des en-t√™tes CSV
        if not contenu_csv or 'header' not in contenu_csv:
            raise ValueError(f"Structure CSV non disponible pour la table '{table_name}'")
        
        colonnes = contenu_csv['header']
        separateur = contenu_csv.get('separator', ';')
        donnees_exemple = contenu_csv.get('data', [])
        
        # Cr√©ation du dictionnaire des variables pour l'inf√©rence de type
        dict_mapping = {}
        if dictionnaire and 'header' in dictionnaire and 'data' in dictionnaire:
            dict_headers = dictionnaire['header']
            dict_data = dictionnaire['data']
            
            # Pour chaque ligne du dictionnaire, cr√©er un mapping nom_colonne -> infos
            for row in dict_data:
                if len(row) >= len(dict_headers):
                    var_info = dict(zip(dict_headers, row))
                    if dict_headers and row:
                        var_name = row[0]
                        dict_mapping[var_name] = var_info
        
        # G√©n√©ration du SQL
        sql_lines = []
        
        # En-t√™te avec informations d√©taill√©es
        sql_lines.extend([
            "-- =====================================================================================",
            f"-- SCRIPT D'IMPORT POUR LA TABLE {nom_table}",
            "-- =====================================================================================",
            f"-- Producteur: {producteur}",
            f"-- Type de donn√©es: {type_donnees}",
            f"-- Sch√©ma: {schema}",
            f"-- Base de donn√©es: {nom_base}",
            f"-- Description: {description}",
            f"-- Mill√©sime: {millesime}",
            f"-- Derni√®re mise √† jour: {date_maj}",
            f"-- Fr√©quence de mise √† jour: {frequence_maj}",
            f"-- G√©n√©r√© le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "-- =====================================================================================",
            ""
        ])
        
        # Suppression de la table existante
        sql_lines.extend([
            "-- Suppression de la table existante (si elle existe)",
            f'DROP TABLE IF EXISTS "{schema}"."{nom_table}";',
            ""
        ])
        
        # Cr√©ation de la table avec inf√©rence de types
        sql_lines.extend([
            "-- Cr√©ation de la table avec types optimis√©s",
            f'CREATE TABLE "{schema}"."{nom_table}" ('
        ])
        
        # G√©n√©ration des d√©finitions de colonnes
        column_definitions = []
        for i, col in enumerate(colonnes):
            # Nettoyage du nom de colonne
            col_clean = col.strip()
            
            # R√©cup√©ration des valeurs d'exemple pour cette colonne
            sample_values = [row[i] if len(row) > i else None for row in donnees_exemple]
            
            # Recherche des informations du dictionnaire
            dict_info = dict_mapping.get(col, {})
            
            # Inf√©rence du type SQL avec toutes les informations disponibles
            sql_type = detect_column_type(
                clean_values=sample_values,
                csv_separator=separateur,
                column_name=col_clean,
                dict_info=dict_info
            )
            
            # Ajout de contraintes sp√©ciales
            constraints = []
            if col_clean.lower() in ['code_insee', 'codgeo', 'id']:
                constraints.append("NOT NULL")
            
            constraint_str = " " + " ".join(constraints) if constraints else ""
            
            # Ajout de la d√©finition de colonne
            column_definitions.append(f'    "{col_clean}" {sql_type}{constraint_str}')
            
            # Debug mode : afficher les d√©tails de l'inf√©rence
            if debug_mode:
                st.write(f"Colonne: {col_clean}")
                st.write(f"Type inf√©r√©: {sql_type}")
                st.write(f"Contraintes: {constraints}")
                st.write("---")
        
        sql_lines.append(",\n".join(column_definitions))
        sql_lines.append(");")
        sql_lines.append("")
        
        conn.close()
        return "\n".join(sql_lines)
        
    except Exception as e:
        if debug_mode:
            st.error(f"Erreur lors de la g√©n√©ration du SQL : {str(e)}")
        raise


def generate_sql_download_button(table_name: str, button_label: str = "üíæ T√©l√©charger le script SQL") -> None:
    """
    G√©n√®re et affiche un bouton de t√©l√©chargement pour le script SQL.
    
    Args:
        table_name: Nom de la table pour laquelle g√©n√©rer le script
        button_label: Texte du bouton de t√©l√©chargement
    """
    sql_script = generate_sql_from_metadata(table_name, debug_mode=False)
    
    if sql_script.startswith("‚ùå"):
        st.error(sql_script)
    else:
        st.download_button(
            label=button_label,
            data=sql_script,
            file_name=f"import_{table_name}.sql",
            mime="text/plain"
        )


def display_sql_generation_interface(table_name: str, debug_mode: bool = True) -> None:
    """
    Affiche l'interface compl√®te de g√©n√©ration SQL avec le script et les boutons.
    
    Args:
        table_name: Nom de la table pour laquelle g√©n√©rer le script
        debug_mode: Si True, affiche les informations de debug
    """
    with st.spinner("G√©n√©ration du script SQL en cours..."):
        sql_script = generate_sql_from_metadata(table_name, debug_mode=debug_mode)
        
        if sql_script.startswith("‚ùå"):
            st.error(sql_script)
        else:
            st.success("üéâ Script SQL g√©n√©r√© avec succ√®s !")
            
            # Affichage du script avec possibilit√© de copier
            st.subheader("üìÑ Script SQL d'import g√©n√©r√©")
            st.code(sql_script, language="sql")
            
            # Bouton de t√©l√©chargement
            st.download_button(
                label="üíæ T√©l√©charger le script SQL",
                data=sql_script,
                file_name=f"import_{table_name}.sql",
                mime="text/plain"
            )
            
            st.info("""
            ### üìã Instructions d'utilisation :
            1. **T√©l√©chargez** le script SQL ci-dessus
            2. **Cr√©ez le sch√©ma** si n√©cessaire : `CREATE SCHEMA IF NOT EXISTS "nom_schema";`
            3. **Importez vos donn√©es** avec une commande COPY adapt√©e √† votre fichier
            4. **Ex√©cutez** le script dans votre outil de gestion PostgreSQL (DBeaver, pgAdmin, etc.)
            """) 